{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face model to GGUF conversion\n",
    "\n",
    "This notebook describes the steps to dowload and convert Hugging Face models to GGUF format so that it is compatible to be imported to Ollama.\n",
    "\n",
    "> This notebook assumes that there llama.cpp repo has been downloaded in another directory.\n",
    "> Ollama should also be installed the system\n",
    "\n",
    "## To download llama.cpp from GitHub\n",
    "\n",
    "``` bash\n",
    "cd <YOUR_GIT_DIRECTORY>\n",
    "git clone https://github.com/ggml-org/llama.cpp\n",
    "```\n",
    "\n",
    "This assumes that you will have the llama.cpp contents at <YOUR_GIT_DIRECTORY>/llama.cpp directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We will have to initialize this notebook's python environment and install necessary python packages from llama.cpp `requirements.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# llama_cpp_dir = \"<YOUR_LLAMA_CPP_DIR>\"\n",
    "llama_cpp_dir = \"/Users/Shared/_ws/llama.cpp\"\n",
    "! pip install -q -r \"{llama_cpp_dir}/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading models from Hugging Face\n",
    "\n",
    "For this sample we're going to download `intfloat/e5-large-v2` embedding model which is not available in Ollama yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/Shared/_models/intfloat/e5-large-v2\n",
      "Hugging Face model directory: /Users/Shared/_models/intfloat/e5-large-v2/hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 20 files: 100%|██████████| 20/20 [00:00<00:00, 8339.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/Shared/_models/intfloat/e5-large-v2/hf'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"intfloat/e5-large-v2\"\n",
    "\n",
    "# For organization, I'd like to organize the directoy in the following manner\n",
    "# root_dir\n",
    "# ├── model_id\n",
    "# │   ├── /hf\n",
    "# │   ├── model_name.q.gguf\n",
    "# │   └── Modelfile.model_name.q\n",
    "root_dir = \"/Users/Shared/_models\"\n",
    "model_dir = os.path.normpath(os.path.join(root_dir, model_id))\n",
    "hf_model_dir = os.path.normpath(os.path.join(model_dir, \"hf\"))\n",
    "\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"Hugging Face model directory: {hf_model_dir}\")\n",
    "\n",
    "snapshot_download(model_id, local_dir=hf_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script path: /Users/Shared/_ws/llama.cpp/convert_hf_to_gguf.py\n",
      "GGUF model directory: /Users/Shared/_models/intfloat/e5-large-v2/e5-large-v2.f32.gguf\n"
     ]
    }
   ],
   "source": [
    "script = os.path.join(llama_cpp_dir, \"convert_hf_to_gguf.py\")\n",
    "print(f\"Script path: {script}\")\n",
    "\n",
    "quant = \"f32\" # full precision\n",
    "model_name = os.path.basename(model_id)\n",
    "gguf_file = f\"{model_name}.{quant}.gguf\"\n",
    "gguf_model_dir = os.path.normpath(os.path.join(model_dir, gguf_file))\n",
    "\n",
    "print(f\"GGUF model directory: {gguf_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: hf\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd_norm.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:token_embd_norm.weight,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:position_embd.weight,            torch.float32 --> F32, shape = {1024, 512}\n",
      "INFO:hf-to-gguf:token_types.weight,              torch.float32 --> F32, shape = {1024, 2}\n",
      "INFO:hf-to-gguf:token_embd.weight,               torch.float32 --> F32, shape = {1024, 30522}\n",
      "INFO:hf-to-gguf:blk.0.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.0.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.1.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.10.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.11.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.12.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.13.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.14.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.15.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.16.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.17.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.18.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.19.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.2.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.20.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.21.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.22.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.bias,         torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,       torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,              torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,            torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.bias,              torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,            torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.23.layer_output_norm.bias,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.layer_output_norm.weight, torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.bias,            torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,          torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.3.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.4.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.5.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.6.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.7.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.8.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output_norm.bias,     torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output_norm.weight,   torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.bias,          torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,        torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,               torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,             torch.float32 --> F32, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.bias,               torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,             torch.float32 --> F32, shape = {1024, 4096}\n",
      "INFO:hf-to-gguf:blk.9.layer_output_norm.bias,    torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.layer_output_norm.weight,  torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.bias,             torch.float32 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,           torch.float32 --> F32, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 512\n",
      "INFO:hf-to-gguf:gguf: embedding length = 1024\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 4096\n",
      "INFO:hf-to-gguf:gguf: head count = 16\n",
      "INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12\n",
      "INFO:hf-to-gguf:gguf: file type = 0\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type unk to 100\n",
      "INFO:gguf.vocab:Setting special token type sep to 102\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "WARNING:gguf.vocab:No handler for special token type cls with id 101 - skipping\n",
      "INFO:gguf.vocab:Setting special token type mask to 103\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/Users/Shared/_models/intfloat/e5-large-v2/e5-large-v2.f32.gguf: n_tensors = 389, total_size = 1.3G\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Writing: 100%|██████████████████████████| 1.34G/1.34G [00:00<00:00, 5.34Gbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /Users/Shared/_models/intfloat/e5-large-v2/e5-large-v2.f32.gguf\n"
     ]
    }
   ],
   "source": [
    "! python \"{script}\" \"{hf_model_dir}\" --outfile \"{gguf_model_dir}\" --outtype \"{quant}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model file for Ollama\n",
    "\n",
    "Refering back to our model directory, we create a `Modelfile` file for Ollama.\n",
    "\n",
    "```\n",
    "root_dir\n",
    "├── model_id\n",
    "│   ├── /hf\n",
    "│   ├── model_name.q.gguf\n",
    "│   └── Modelfile.model_name.q\n",
    "```\n",
    "\n",
    "We'll name it with the following format: `Modelfile.<MODEL_NAME>.<QUANT>`. And then write the following contents:\n",
    "\n",
    "``` text\n",
    "FROM {gguf_model_dir}\n",
    "```\n",
    "\n",
    "For more information, refer to Ollama documentation at https://github.com/ollama/ollama/blob/main/docs/modelfile.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = f\"Modelfile.{model_name}.{quant}\"\n",
    "modelfile_dir = os.path.normpath(os.path.join(model_dir, modelfile))\n",
    "\n",
    "with open(modelfile_dir, \"w\") as f:\n",
    "    f.write(f\"FROM {gguf_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model into Ollama\n",
    "\n",
    "Now that we have a Modelfile, we can now import the model to Ollama using the following command\n",
    "\n",
    "``` bash\n",
    "ollama create <MODEL_NAME> -f <GGUF_MODEL_DIR>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   ID              SIZE      MODIFIED    \n",
      "llama3.2:latest                        a80c4f17acd5    2.0 GB    6 weeks ago    \n",
      "nomic-embed-text:latest                0a109f422b47    274 MB    6 weeks ago    \n",
      "deepseek-r1:8b-llama-distill-q4_K_M    28f8fd6cdc67    4.9 GB    7 weeks ago    \n",
      "llama3.1:latest                        46e0c10c039e    4.9 GB    7 weeks ago    \n",
      "deepseek-r1:latest                     0a8c26691023    4.7 GB    7 weeks ago    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# For reference I have the following models in Ollama\n",
    "\n",
    "! ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 0% ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 4% ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 18% ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 32% ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 39% ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 52% ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 66% ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 72% ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 86% ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:da0fce4302df4c14caa6a938e9b513dedebe740938eafae2bc61432971afaa11 \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "! ollama create \"{model_name}\" -f \"{modelfile_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   ID              SIZE      MODIFIED       \n",
      "e5-large-v2:latest                     cfb0e6095fc4    1.3 GB    11 seconds ago    \n",
      "llama3.2:latest                        a80c4f17acd5    2.0 GB    6 weeks ago       \n",
      "nomic-embed-text:latest                0a109f422b47    274 MB    6 weeks ago       \n",
      "deepseek-r1:8b-llama-distill-q4_K_M    28f8fd6cdc67    4.9 GB    7 weeks ago       \n",
      "llama3.1:latest                        46e0c10c039e    4.9 GB    7 weeks ago       \n",
      "deepseek-r1:latest                     0a8c26691023    4.7 GB    7 weeks ago       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Lets check if the model is in Ollama\n",
    "\n",
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the model\n",
    "\n",
    "Lets try to use the model. I downloaded an embedding model so we're going to use the embeddings endpoint of Ollama. Our expectation is that it should generate the vector with a length of 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/embeddings\"\n",
    "payload = json.dumps({\n",
    "    \"model\": model_name,\n",
    "    \"prompt\": \"The quick brown fox jumps over the lazy dog.\"\n",
    "})\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "json_response = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1024\n",
    "len(json_response[\"embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of both models\n",
    "\n",
    "Since we downloaded an embedding model, we can compare the results of both models i.e. Using Hugging Face libraries and results from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copied from \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Each input text should start with \"query: \" or \"passage: \".\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "input_texts = ['query: The lazy dog was jumped over by the quick brown fox.']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_dir)\n",
    "model = AutoModel.from_pretrained(hf_model_dir)\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "embeddings = embeddings.T\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92337578])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Result from Ollama\n",
    "A = np.array(json_response[\"embedding\"])\n",
    "B = embeddings.detach().numpy()\n",
    "\n",
    " \n",
    "# compute cosine similarity\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good result! so we know they have similar performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
